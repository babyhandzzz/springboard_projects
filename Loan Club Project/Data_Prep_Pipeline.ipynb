{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from feature_engine import outlier_removers \n",
    "from feature_engine.categorical_encoders import OneHotCategoricalEncoder, RareLabelCategoricalEncoder\n",
    "\n",
    "# Display options\n",
    "\n",
    "pd.options.mode.chained_assignment = None #set it to None to remove SettingWithCopyWarning\n",
    "pd.options.display.float_format = '{:.4f}'.format #set it to convert scientific noations such as 4.225108e+11 to 422510842796.00\n",
    "pd.set_option('display.max_columns', 100) #  display all the columns\n",
    "pd.set_option('display.max_rows', 100) # display all the rows\n",
    "np.set_printoptions(suppress=True,formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "\n",
    "def remove_single_unique_value_features(dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    Drop all the columns that only contain one unique value.\n",
    "    not optimized for categorical features yet.\n",
    "    \n",
    "    \"\"\"    \n",
    "    cols_to_drop = dataframe.nunique()\n",
    "    cols_to_drop = cols_to_drop.loc[cols_to_drop.values==1].index\n",
    "    dataframe = dataframe.drop(cols_to_drop,axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df is loaded\n",
      "(2260668, 61)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Data/loan.csv',low_memory=False)\n",
    "print('df is loaded')\n",
    "\n",
    "# the list contains features that are either proven useless or introduce look-ahead bias into data.\n",
    "list_to_remove = ['last_pymnt_amnt','total_rec_prncp','total_pymnt',\n",
    "                  'total_pymnt_inv','total_rec_int','total_rec_late_fee','total_rec_prncp',\n",
    "                  'issue_d','earliest_cr_line','last_pymnt_d',\n",
    "                  'last_credit_pull_d','id','member_id','settlement_date',\n",
    "                  'next_pymnt_d','zip_code',\n",
    "                  \n",
    "                 'delinq_2yrs','mths_since_last_delinq', 'pub_rec',\n",
    " 'recoveries',\n",
    " 'collection_recovery_fee',\n",
    " 'collections_12_mths_ex_med',\n",
    " 'acc_now_delinq',\n",
    " 'tot_coll_amt',\n",
    " 'open_acc_6m',\n",
    " 'open_act_il',\n",
    " 'open_il_12m',\n",
    " 'open_il_24m',\n",
    " 'mths_since_rcnt_il',\n",
    " 'total_bal_il',\n",
    " 'il_util',\n",
    " 'open_rv_12m',\n",
    " 'open_rv_24m',\n",
    " 'max_bal_bc',\n",
    " 'all_util',\n",
    " 'inq_fi',\n",
    " 'total_cu_tl',\n",
    " 'inq_last_12m',\n",
    " 'chargeoff_within_12_mths',\n",
    " 'delinq_amnt',\n",
    " 'mths_since_recent_revol_delinq',\n",
    " 'num_accts_ever_120_pd',\n",
    " 'num_tl_120dpd_2m',\n",
    " 'num_tl_30dpd',\n",
    " 'num_tl_90g_dpd_24m',\n",
    " 'pub_rec_bankruptcies',\n",
    " 'tax_liens']\n",
    "\n",
    "df.drop(list_to_remove,axis='columns',inplace=True)\n",
    "\n",
    "df = df.infer_objects()\n",
    "# drop any features that have more than 30% of NaN values in them.\n",
    "df.dropna(axis=1,how='any',thresh=int(0.3*len(df)),inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Manipulations\n",
    "* Specific to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid    0.7991\n",
       "Charged Off   0.2009\n",
       "Name: loan_status, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_drop = ['Current','Late (31-120 days)','Late (16-30 days)','In Grace Period','Default']\n",
    "df = df[~df.loan_status.isin(labels_to_drop)]\n",
    "\n",
    "dictionary = {'Does not meet the credit policy. Status:Fully Paid':'Fully Paid',\n",
    "             'Does not meet the credit policy. Status:Charged Off':'Charged Off'}\n",
    "\n",
    "df['loan_status'].replace(dictionary,inplace=True)\n",
    "df['loan_status'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306356, 57)\n"
     ]
    }
   ],
   "source": [
    "# delete all the columns that contain single unique values\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        df.drop(col,inplace=True,axis=1)\n",
    "print(df.shape)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['loan_status']].values.ravel()\n",
    "X = df.drop('loan_status',axis='columns')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, \n",
    "                                            random_state=42, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing data into categorical and numerical parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical part:\n",
      "(979767, 41)\n",
      "(326589, 41)\n",
      "Categorical part:\n",
      "(979767, 15)\n",
      "(326589, 15)\n"
     ]
    }
   ],
   "source": [
    "# dividing training and testing data into categorical and numerical parts\n",
    "nmrcl_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "nmrcl_X_test = X_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "ctgrcl_X_train = X_train.select_dtypes(include=['object'])\n",
    "ctgrcl_X_test = X_test.select_dtypes(include=['object'])\n",
    "\n",
    "\n",
    "print('Numerical part:')\n",
    "print(nmrcl_X_train.shape)\n",
    "print(nmrcl_X_test.shape)\n",
    "print('Categorical part:')\n",
    "print(ctgrcl_X_train.shape)\n",
    "print(ctgrcl_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating numerical data\n",
    "* starting with pd.fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Training df medians have to be saved as a pd.Series object othervise replace() \n",
    "method does not work when replacing NaN in testing df.\n",
    "\"\"\"\n",
    "\n",
    "training_medians = pd.Series(nmrcl_X_train.median()) # get the training medians \n",
    "\n",
    "nmrcl_X_train = nmrcl_X_train.fillna(training_medians) # fillna first\n",
    "nmrcl_X_test = nmrcl_X_test.fillna(training_medians)\n",
    "\n",
    "#nmrcl_X_train = remove_single_unique_value_features(nmrcl_X_train) # remove features with \n",
    "#nmrcl_X_test = remove_single_unique_value_features(nmrcl_X_test)   # single unique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining numerical features treatment\n",
    "* I actually don't know if it's a good idea, but if we pipeline, we decrease the number of points of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:184: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:184: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 14 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:184: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 22 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:184: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 25 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:184: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "/Users/babyhandzzz/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 34 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    }
   ],
   "source": [
    "# 1.outlier replacement\n",
    "# 2.discretization\n",
    "# 3.scaling\n",
    "\n",
    "capper = outlier_removers.Winsorizer(distribution='skewed', tail='both', fold=1.5)\n",
    "discretizer = KBinsDiscretizer(n_bins=12, encode='ordinal', strategy='kmeans')\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_pipeline = Pipeline([('capper',capper),\n",
    "                    ('discretizer',discretizer),\n",
    "                    ('scaler',scaler)])\n",
    "\n",
    "nmrcl_X_train = numerical_pipeline.fit_transform(nmrcl_X_train)\n",
    "nmrcl_X_test = numerical_pipeline.transform(nmrcl_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast produced np.arrays back to pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the way to access names of the columns, it's needed to convert \n",
    "# pipeline-produced np.array back to pd.DataFrame.\n",
    "nmrc_feature_cols = numerical_pipeline.named_steps['capper'].variables\n",
    "\n",
    "nmrcl_X_train = pd.DataFrame(nmrcl_X_train, columns=nmrc_feature_cols)\n",
    "nmrcl_X_test = pd.DataFrame(nmrcl_X_test, columns=nmrc_feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining categorical features treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgrcl_X_train.fillna('other',inplace=True)\n",
    "ctgrcl_X_test.fillna('other',inplace=True)\n",
    "\n",
    "# two step pipeline:\n",
    "# 1. rare labels (frequency below 1% are changed to 'rare')\n",
    "# 2. n-1 OneHot encoding\n",
    "\n",
    "encoder = RareLabelCategoricalEncoder(tol=0.01)\n",
    "ohe_enc = OneHotCategoricalEncoder(top_categories=None,drop_last=True)\n",
    "\n",
    "categorical_pipeline = Pipeline([('rare_label',encoder),('onehot',ohe_enc)])\n",
    "\n",
    "ctgrcl_X_train = categorical_pipeline.fit_transform(ctgrcl_X_train)\n",
    "ctgrcl_X_test = categorical_pipeline.transform(ctgrcl_X_test)\n",
    "\n",
    "# reseting the index so all the dfs are alinable\n",
    "ctgrcl_X_train.reset_index(drop=True,inplace=True)\n",
    "ctgrcl_X_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast encoded labels back to a dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LabelEncoder() output is a numpy array, it's missing the index which is later used for\n",
    "concatanation of categorical, numercial and label data together. The following is a \n",
    "primitive solution but it works and there is no missalignment in the final df.\n",
    "\n",
    "\"\"\"\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "y_train.columns = ['training labels']\n",
    "y_test.columns = ['testing labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking all the dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(979767, 143)\n",
      "(326589, 143)\n"
     ]
    }
   ],
   "source": [
    "final_train = pd.concat([nmrcl_X_train,ctgrcl_X_train,y_train],axis=1)\n",
    "final_test = pd.concat([nmrcl_X_test,ctgrcl_X_test,y_test],axis=1)\n",
    "\n",
    "print(final_train.shape)\n",
    "print(final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.to_csv('Data/decreased_size_train.csv')\n",
    "final_test.to_csv('Data/decreased_size_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
