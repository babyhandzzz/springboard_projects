{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595522702107",
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOBIDEV \n",
    "## *Test task for Data Scientist position*\n",
    "### candidate: Dmitri Kochubei\n",
    "### email: dmitri.kochubei@gmail.com\n",
    "___\n",
    "1. *Provide with useful insights from the data* - \n",
    "\n",
    "This part requires further investigation, however there is a problem with, I beleive, 607 entries in the training dataset. review_scores are missing for those entries.\n",
    "___\n",
    "2. *Find the top impact factors on the price in the data* - \n",
    "\n",
    "Full list of top impact factors can be viewed at the very buttom of this notebook. But here is the list of the most impactful features based on Gradient Boosting Regressor model:\n",
    "\n",
    "    accommodates - max # of people to reside \n",
    "    bathrooms\n",
    "    cleaning_fee\n",
    "    beds \n",
    "    description_beds \n",
    "    number_of_reviews\n",
    "    host_acceptance_rate\n",
    "    rm_tp_entirehomeapt - room type (Entire Apartment)\n",
    "\n",
    "There are no surprises in these features, They can be broken down into 2 categories:\n",
    "    \n",
    "    description of a place (size, type, amenities)\n",
    "    description of a host \n",
    "    \n",
    "So the price mostly depends on the qualities of the apartment and renter's behavior.\n",
    "___\n",
    "\n",
    "3. *Select the best metric for the model evaluation and justify the choice* - \n",
    "    \n",
    "    For now the stanard MSE is chosen, as the most commonly used criteria for regression models. The choice may change in the future, after more careful evaluation.\n",
    "___    \n",
    "5. *Describe the business value of your results* - \n",
    "\n",
    "I'm not really familiar with the context of the dataset, so it's hard to think of specific applications. But let's say, a host on the platform undervalues or overvalues her listing (given that the host has a choice). Using ML model the platform could nudge that host to adjust the price for higher booking rate.\n",
    "____\n",
    "*Final thoughts* - \n",
    "\n",
    "There is a lot more to be done with this dataset:\n",
    "    \n",
    "    MUCH more EDA, there is hardly anything done due to time limits\n",
    "    Model tuning\n",
    "    ...\n",
    "    \n",
    "But I had to stop somewhere.\n",
    "\n",
    "\n",
    "If the dataset is big enough I would try deep learning.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic of this notebook\n",
    "1. A number of functions is defiend to pre-process and or encode all the features.\n",
    "2. 2 Datasest (training and testing) are compiled for modeling.\n",
    "3. A number of Models is tested.\n",
    "4. Resulting predictions are generated using the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processed dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "35\n34\n"
    }
   ],
   "source": [
    "columns = [\n",
    "'summary', # text tfidf\n",
    "'description', # text tfidf\n",
    "'neighborhood_overview', # text tfidf\n",
    "'transit', # text tfidf\n",
    "'host_about', # text tfidf\n",
    "\n",
    "'host_since', # date, number of months\n",
    "\n",
    "'host_verifications', # string of terms\n",
    "'amenities', # string of terms\n",
    "\n",
    "'host_response_rate', # string, strip the % sign\n",
    "'host_acceptance_rate', # string, strip the % sign\n",
    "'cleaning_fee', # string, strip the $ sign\n",
    "\n",
    "'zipcode',  # string, probably one of the most important features\n",
    "'property_type', # string\n",
    "'room_type', # string\n",
    "'accommodates', #int\n",
    "'bathrooms', # float\n",
    "'bedrooms', # float\n",
    "'beds', # float\n",
    "'bed_type', # string ohe\n",
    "'guests_included', # int, ohe, anything that is below 1% probably can go\n",
    "'minimum_nights', # int, ohe, anything that is below 1% probably can go\n",
    "'maximum_nights', # int, ohe, anything that is below 1% probably can go\n",
    "'number_of_reviews', # int, ohe, anything that is below 1% probably can go\n",
    "'review_scores_rating', # float, do nothing for now\n",
    "'review_scores_accuracy', #float,\n",
    "'review_scores_cleanliness', #float,\n",
    "'review_scores_checkin', #float,\n",
    "'review_scores_communication', #float,\n",
    "'review_scores_location', #float,\n",
    "'review_scores_value', #float, \n",
    "'instant_bookable', # str t/f, bianry encoding\n",
    "'cancellation_policy', # str ,4 categories, ohe\n",
    "'calculated_host_listings_count', #  int, ohe\n",
    "'reviews_per_month',# float\n",
    "\n",
    "'price' # target variable\n",
    "]\n",
    "\n",
    "print(len(columns))\n",
    "data = pd.read_csv('/Users/babyhandzzz/Desktop/ELEPH@NT/Datasets/mobidev/data/Train.csv',usecols=columns)\n",
    "columns = [i for i in columns if i != 'price'] \n",
    "print(len(columns))\n",
    "test_data = pd.read_csv('/Users/babyhandzzz/Desktop/ELEPH@NT/Datasets/mobidev/data/Submission.csv',usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF transfomrmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_columns(df,columns):\n",
    "    \n",
    "    list_ = []\n",
    "    new_df = pd.DataFrame(index=range(df.shape[0]))\n",
    "    tfidf =TfidfVectorizer(min_df=40, stop_words='english', lowercase=True, max_df=500)\n",
    "     \n",
    "    for column in columns:\n",
    "        df[column].fillna('none',inplace=True) \n",
    "        processed_text = tfidf.fit(data[column]) # fitting on original dataset    \n",
    "        processed_text = tfidf.transform(df[column])\n",
    "        processed_text = pd.DataFrame(processed_text.toarray(), columns=tfidf.get_feature_names())\n",
    "        processed_text.columns = [column+'_'+i for i in processed_text.columns]\n",
    "        \n",
    "        list_.append(processed_text)\n",
    "    \n",
    "    df = pd.concat(list_,axis=1)\n",
    "    return df\n",
    "\n",
    "text_columns = [\n",
    "'summary', # text\n",
    "'description', # text\n",
    "'neighborhood_overview', # text\n",
    "'transit', # text\n",
    "'host_about'] # text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of months listed on platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def host_since(df):\n",
    "    df = df[['host_since']]\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'])\n",
    "    df['now'] =  pd.Timestamp('2020-07-22')\n",
    "    df = pd.DataFrame((df['now'] - df['host_since'])/np.timedelta64(1, 'M'), columns=['months_active'])\n",
    "    df['months_active'] = df['months_active'].astype('int')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amenities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amenities(df, column):\n",
    "    df[column] = df[column].str.replace(r'{|\"|}|/', '')\n",
    "    df[column] = df[column].str.replace(' ', '')\n",
    "    vector = CountVectorizer()\n",
    "    vector.fit(data['amenities']) # this solutions does not generalize well but it's good enough for now\n",
    "    series = vector.transform(df[column])\n",
    "    df = pd.DataFrame(series.toarray(), columns=vector.get_feature_names())\n",
    "    df.columns = [column+'_'+i for i in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(df,column,prefix,rare=False):\n",
    "    \n",
    "    df[column] = df[column].str.replace(\" \",\"\")\n",
    "    df[column] = df[column].str.replace(r'-|&|/', '')\n",
    "    \n",
    "    most_frequent_value = data[column].value_counts().idxmax() # most frequent value, has to be from the training dataset\n",
    "    df[column].fillna(most_frequent_value,inplace=True)\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(data[column]) # training data\n",
    "    \n",
    "    if rare==False:\n",
    "        series = vectorizer.transform(df[column])\n",
    "        df = pd.DataFrame(series.toarray(),columns=vectorizer.get_feature_names())\n",
    "        df.columns = [prefix+i for i in df.columns]\n",
    "\n",
    "    elif rare==True:\n",
    "        series = df[column].mask(df[column].map(df[column].value_counts(normalize=True)) < 0.01, 'other')\n",
    "        df = pd.DataFrame(series,columns=[column])\n",
    "        series = vectorizer.transform(df[column])\n",
    "        df = pd.DataFrame(series.toarray(),columns=vectorizer.get_feature_names())\n",
    "        df.columns = [prefix+i for i in df.columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaN replacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(df, column):\n",
    "    most_frequent_value = df[column].value_counts().idxmax()\n",
    "    df[column].fillna(most_frequent_value,inplace=True)\n",
    "    df = df[[column]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCT(%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pct(df, column):\n",
    "    df = pd.DataFrame(df[column].str.replace(\"%\",\"\"),columns=[column])\n",
    "    most_frequent_value = df[column].value_counts().idxmax()\n",
    "    df[column].fillna(most_frequent_value,inplace=True)\n",
    "    df[column] = df[column].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning_fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_fee(df, column):\n",
    "    df = pd.DataFrame(df[column].str.replace(\"$\",\"\"),columns=[column])\n",
    "    df.fillna(0,inplace=True)\n",
    "    df[column] = df[column].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target_pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_price(df,column):\n",
    "    series = df[column].str.replace(r'$','')\n",
    "    series = series.str.replace(r',','')\n",
    "    df = pd.DataFrame(series,columns=[column])\n",
    "    df['price'] = pd.to_numeric(df['price'])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_num_features(df,num_cols): # unlike most of the other functions here, this one takes a list of columns.\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(ml_df[num_cols]) # fitting on training dataset\n",
    "    scaled = scaler.transform(df[num_cols])\n",
    "    scaled_df = pd.DataFrame(scaled,columns=num_cols)\n",
    "    df[df.columns[:20]] = scaled_df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging it all together (Training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [\n",
    "replace_nan(data, 'beds'), # scale\n",
    "replace_nan(data, 'bathrooms'), # scale\n",
    "host_since(data), # scale\n",
    "data[['accommodates']], # scale\n",
    "data[['guests_included']], # scale\n",
    "data[['minimum_nights']],# scale\n",
    "data[['maximum_nights']], # scale\n",
    "data[['number_of_reviews']], # scale\n",
    "replace_nan(data, 'review_scores_rating'), # scale\n",
    "replace_nan(data, 'review_scores_accuracy'), # scale\n",
    "replace_nan(data, 'review_scores_cleanliness'), # scale\n",
    "replace_nan(data, 'review_scores_checkin'), # scale\n",
    "replace_nan(data, 'review_scores_communication'), # scale\n",
    "replace_nan(data, 'review_scores_location'),# scale\n",
    "replace_nan(data, 'review_scores_value'), # scale\n",
    "data[['calculated_host_listings_count']], # scale\n",
    "replace_nan(data,'reviews_per_month'), # scale\n",
    "remove_pct(data, 'host_response_rate'),# scale\n",
    "remove_pct(data, 'host_acceptance_rate'),# scale\n",
    "cleaning_fee(data, 'cleaning_fee'), # scale\n",
    "##################################\n",
    "tfidf_columns(data,text_columns),\n",
    "amenities(data, 'amenities'),\n",
    "ohe(data,'zipcode','zip_',False),\n",
    "ohe(data,'property_type', 'pr_tp_', rare=True),\n",
    "ohe(data,'room_type','rm_tp_',False),\n",
    "ohe(data,'bed_type','bd_tp_',True),\n",
    "ohe(data,'cancellation_policy','canc_pol_',False),\n",
    "data['instant_bookable'].replace({'f':0,'t':1}),\n",
    "##################################\n",
    "pre_proc_price(data,'price')\n",
    "]\n",
    "\n",
    "\n",
    "ml_df = pd.concat(funcs, axis=1)\n",
    "numerical_columns = ml_df.columns[:20]\n",
    "ml_df = scale_num_features(ml_df, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging it all together (Testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs_test = [\n",
    "replace_nan(test_data, 'beds'), # scale\n",
    "replace_nan(test_data, 'bathrooms'), # scale\n",
    "host_since(test_data), # scale\n",
    "test_data[['accommodates']], # scale\n",
    "test_data[['guests_included']], # scale\n",
    "test_data[['minimum_nights']],# scale\n",
    "test_data[['maximum_nights']], # scale\n",
    "test_data[['number_of_reviews']], # scale\n",
    "replace_nan(test_data, 'review_scores_rating'), # scale\n",
    "replace_nan(test_data, 'review_scores_accuracy'), # scale\n",
    "replace_nan(test_data, 'review_scores_cleanliness'), # scale\n",
    "replace_nan(test_data, 'review_scores_checkin'), # scale\n",
    "replace_nan(test_data, 'review_scores_communication'), # scale\n",
    "replace_nan(test_data, 'review_scores_location'),# scale\n",
    "replace_nan(test_data, 'review_scores_value'), # scale\n",
    "test_data[['calculated_host_listings_count']], # scale\n",
    "replace_nan(test_data,'reviews_per_month'), # scale\n",
    "remove_pct(test_data, 'host_response_rate'),# scale\n",
    "remove_pct(test_data, 'host_acceptance_rate'),# scale\n",
    "cleaning_fee(test_data, 'cleaning_fee'), # scale\n",
    "##################################\n",
    "tfidf_columns(test_data,text_columns),\n",
    "amenities(test_data, 'amenities'),\n",
    "ohe(test_data,'zipcode','zip_',False),\n",
    "ohe(test_data,'property_type', 'pr_tp_', rare=True),\n",
    "ohe(test_data,'room_type','rm_tp_',False),\n",
    "ohe(test_data,'bed_type','bd_tp_',True),\n",
    "ohe(test_data,'cancellation_policy','canc_pol_',False),\n",
    "test_data['instant_bookable'].replace({'f':0,'t':1})\n",
    "]\n",
    "\n",
    "test_df = pd.concat(funcs_test, axis=1)\n",
    "test_df = scale_num_features(test_df, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ml_df.columns[:-1]\n",
    "X = ml_df[features]\n",
    "y = ml_df[['price']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "#######\nLinearRegression()\nTrain_score: 0.9986877332995239\nTest_score: -442.89723567638276\n#######\nGradientBoostingRegressor()\nTrain_score: 0.8920944194390308\nTest_score: 0.3398819280322709\n#######\nElasticNet()\nTrain_score: 0.34698171501710595\nTest_score: 0.35739312949075075\n#######\nLasso()\nTrain_score: 0.38796424692733034\nTest_score: 0.3753925166717226\n#######\nDecisionTreeRegressor()\nTrain_score: 1.0\nTest_score: -1.0303513219524203\n"
    }
   ],
   "source": [
    "models = [LinearRegression(),GradientBoostingRegressor(),ElasticNet(),Lasso(),DecisionTreeRegressor()]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train,y_train)\n",
    "    print('#######')\n",
    "    print(model)\n",
    "    print('Train_score: {}'.format(model.score(X_train,y_train)))\n",
    "    print('Test_score: {}'.format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrowing down to Lasso and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8666150baf71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#######'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "models = [Lasso(alpha=2,max_iter=1000),GradientBoostingRegressor(n_estimators=2000, min_samples_leaf=20)]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train,y_train)\n",
    "    print('#######')\n",
    "    print(model)\n",
    "    print('Train_score: {}'.format(model.score(X_train,y_train)))\n",
    "    print('Test_score: {}'.format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d78e28b6a7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msanity_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Predictions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Actual Price\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difference'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predictions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Actual Price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = pd.Series(models[1].predict(X))\n",
    "sanity_check = pd.DataFrame({'Predictions':predictions,\"Actual Price\": y['price']})\n",
    "sanity_check['difference'] = sanity_check['Predictions'] - sanity_check['Actual Price']\n",
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Impact Factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotFittedError",
     "evalue": "This GradientBoostingRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3cb0652fe541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'importance'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeature_importance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'importance'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \"\"\"\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         relevant_trees = [tree\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_check_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;34m\"\"\"Check that the estimator is initialized, raising an error if not.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This GradientBoostingRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame({'importance':models[1].feature_importances_, 'feature':X.columns})\n",
    "feature_importance.sort_values('importance',ascending=False)[:20]"
   ]
  }
 ]
}