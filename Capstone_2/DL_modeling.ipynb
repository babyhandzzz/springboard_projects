{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22EOhg8BbN4v",
        "colab_type": "text"
      },
      "source": [
        "# Google Colab related imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNkpBax3YGuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a39f4c0e-6813-4c2a-af76-a2811a923d6c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02TNayLWVFLQ",
        "colab_type": "text"
      },
      "source": [
        "# Core imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNsVFIuzGxTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, LSTM, Conv1D, MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.models import load_model# save entire model to HDF5 \n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9-kD4fcUeBc",
        "colab_type": "text"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPTUKGdtH46W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/imdb_master.csv'\n",
        "dataframe = pd.read_csv(data_path, encoding='latin1',usecols=['review','label'])\n",
        "dataframe = dataframe.loc[dataframe.label != 'unsup']\n",
        "dataframe.label.replace({'neg':0,'pos':1},inplace=True)\n",
        "X = dataframe[['review']]\n",
        "y = dataframe[['label']]\n",
        "vec_size = 300"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKSuDkS9bvR2",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2WfFR0ZUsmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "9131be00-8399-4abb-8ad3-75544fbd3b6f"
      },
      "source": [
        "#1.Remove any punctuation\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans(\"\",\"\", string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "#2. Remove stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "    return \" \".join(text)\n",
        "\n",
        "X.review = X.review.map(lambda x: remove_punct(x))\n",
        "X.review = X.review.map(remove_stopwords)\n",
        "\n",
        "# 3. Label Encoding\n",
        "y = pd.DataFrame(to_categorical(np.asarray(y['label'])))\n",
        "\n",
        "# 4. Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
        "                                                    random_state=16)\n",
        "\n",
        "one_d_y_test = [1 if item == 1 else 0 for item in y_test[1]] # 1d vector of predictions for classification report\n",
        "\n",
        "X_train.reset_index(drop=True,inplace=True)\n",
        "X_test.reset_index(drop=True,inplace=True)\n",
        "y_train.reset_index(drop=True,inplace=True)\n",
        "y_test.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbdb4hzucA8f",
        "colab_type": "text"
      },
      "source": [
        "#Keras preparation\n",
        "##1.Tokenizer \n",
        "##2.Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfGuijK3VnqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train.review)\n",
        "word_index = tokenizer.word_index \n",
        "vocab_size = len(word_index)+1\n",
        "max_length = 260  #!\n",
        "\n",
        "# 1. Train tokenizer&padding\n",
        "train_sequence = tokenizer.texts_to_sequences(X_train.review)\n",
        "train_padded = pad_sequences(train_sequence, maxlen=max_length, padding='post',\n",
        "                             truncating='post')\n",
        "# 2. Test tokenizer&padding\n",
        "test_sequence = tokenizer.texts_to_sequences(X_test.review)\n",
        "test_padded = pad_sequences(test_sequence, maxlen=max_length, padding='post',\n",
        "                            truncating='post')\n",
        "\n",
        "# Making sure reverse operation produces the inverse of encoding\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode(text):\n",
        "    return \" \".join([reverse_word_index.get(i,\"?\") for i in text]) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6cc0e3BcQIS",
        "colab_type": "text"
      },
      "source": [
        "# Parse the glove file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOo_i0YBVu-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e12666f-37e1-4b59-e1b0-da6f70dd2434"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_path, 'glove.42B.300d.txt'))\n",
        "for line in f:\n",
        "    # split every value on space\n",
        "    values = line.split()\n",
        "    # the word itself is the first item in every line\n",
        "    word = values[0]\n",
        "    # the actual vector is the vector that follows the word\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    # dictionary mapping of the words to their vectors\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1917494 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsScCF0Qc7i1",
        "colab_type": "text"
      },
      "source": [
        "# Create Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P_sszVZVw6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.array matrix that \n",
        "embedding_matrix = np.zeros((vocab_size, vec_size))\n",
        "                # word index is a dictionary that contains tokenized words from the dataset\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vWM1FwJ2_xR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.load('/content/drive/My Drive/Colab Notebooks/embedding_matrix.npy')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZf_CRDh1fG9",
        "colab_type": "text"
      },
      "source": [
        "# Function to generate perforemance metrics stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ6HpND81ecz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance_metrics(model, y_test):\n",
        "    \n",
        "    predicitons = model.predict_classes(test_padded)\n",
        "    class_report = classification_report(y_test, predictions)\n",
        "    conf_matrix = pd.DataFrame(confusion_matrix(one_d_y_test, predictions), \n",
        "                                    index=labels, columns=labels)\n",
        "    print(class_report)\n",
        "\n",
        "    return  conf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mymtLCBJBBgV",
        "colab_type": "text"
      },
      "source": [
        "# DL Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r789kzRrCd7z",
        "colab_type": "text"
      },
      "source": [
        "## 1.Plain vanilla LSTM \n",
        "- GloVe embeddings are trainable weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwnlt7KiBS4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 12\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgMJI7PI164Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "585db200-6b70-45e0-e6bc-47288a553220"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=vec_size,input_length=max_length,\n",
        "weights=[embedding_matrix],trainable=False))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, \n",
        "                                      restore_best_weights=True,verbose=1)\n",
        "\n",
        "model.fit(train_padded, y_train, validation_split=0.1,epochs=EPOCHS, \n",
        "          batch_size=BATCH_SIZE, callbacks=[es], verbose=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 260, 300)          44910300  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 45,003,870\n",
            "Trainable params: 93,570\n",
            "Non-trainable params: 44,910,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "247/247 [==============================] - 127s 514ms/step - loss: 0.6787 - accuracy: 0.5546 - val_loss: 0.6943 - val_accuracy: 0.4923\n",
            "Epoch 2/12\n",
            "247/247 [==============================] - 131s 532ms/step - loss: 0.6895 - accuracy: 0.5091 - val_loss: 0.6871 - val_accuracy: 0.5291\n",
            "Epoch 3/12\n",
            "247/247 [==============================] - 130s 527ms/step - loss: 0.6815 - accuracy: 0.5298 - val_loss: 0.6853 - val_accuracy: 0.5049\n",
            "Epoch 4/12\n",
            "247/247 [==============================] - 127s 513ms/step - loss: 0.6640 - accuracy: 0.5682 - val_loss: 0.6567 - val_accuracy: 0.5803\n",
            "Epoch 5/12\n",
            "247/247 [==============================] - 127s 514ms/step - loss: 0.6258 - accuracy: 0.6626 - val_loss: 0.6342 - val_accuracy: 0.7091\n",
            "Epoch 6/12\n",
            "247/247 [==============================] - 126s 511ms/step - loss: 0.5766 - accuracy: 0.7198 - val_loss: 0.5679 - val_accuracy: 0.7326\n",
            "Epoch 7/12\n",
            "247/247 [==============================] - 126s 512ms/step - loss: 0.5471 - accuracy: 0.7603 - val_loss: 0.5461 - val_accuracy: 0.7606\n",
            "Epoch 8/12\n",
            "247/247 [==============================] - 129s 523ms/step - loss: 0.5176 - accuracy: 0.7770 - val_loss: 0.5101 - val_accuracy: 0.7914\n",
            "Epoch 9/12\n",
            "247/247 [==============================] - 125s 507ms/step - loss: 0.6785 - accuracy: 0.5396 - val_loss: 0.6878 - val_accuracy: 0.5220\n",
            "Epoch 10/12\n",
            "247/247 [==============================] - 128s 516ms/step - loss: 0.6621 - accuracy: 0.5560 - val_loss: 0.6143 - val_accuracy: 0.6663\n",
            "Epoch 11/12\n",
            "247/247 [==============================] - ETA: 0s - loss: 0.6429 - accuracy: 0.5924Restoring model weights from the end of the best epoch.\n",
            "247/247 [==============================] - 128s 517ms/step - loss: 0.6429 - accuracy: 0.5924 - val_loss: 0.5706 - val_accuracy: 0.7329\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d83616f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoPT0j2jgqUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "b026d4ce-1d91-4afd-f051-ae5e8bc26fbc"
      },
      "source": [
        "def collect_metrics(model):\n",
        "    predictions = model.predict_classes(test_padded)\n",
        "    class_report = classification_report(one_d_y_test, predictions)\n",
        "    fpr, tpr, thresholds = roc_curve(one_d_y_test, predictions)\n",
        "    auc_ = auc(fpr, tpr)\n",
        "    \n",
        "    print(class_report)\n",
        "    print(auc_)\n",
        "\n",
        "collect_metrics(model_3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82      7494\n",
            "           1       0.83      0.79      0.81      7506\n",
            "\n",
            "    accuracy                           0.81     15000\n",
            "   macro avg       0.81      0.81      0.81     15000\n",
            "weighted avg       0.81      0.81      0.81     15000\n",
            "\n",
            "0.8112875058906704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHlrz7LnhTH",
        "colab_type": "text"
      },
      "source": [
        "# Conv1D NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42JVEZyK240d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "162b7ba0-9041-4f44-a587-86a38f1f8e47"
      },
      "source": [
        "model_2 = Sequential()\n",
        "model_2.add(Embedding(input_dim=vocab_size, output_dim=vec_size,input_length=max_length,\n",
        "weights=[embedding_matrix],trainable=False))\n",
        "model_2.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "model_2.add(MaxPooling1D(pool_size=2))\n",
        "model_2.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "model_2.add(MaxPooling1D(pool_size=2))\n",
        "model_2.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "model_2.add(MaxPooling1D(pool_size=2))\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(256, activation='relu'))\n",
        "model_2.add(Dense(2, activation='softmax'))\n",
        "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_2.summary()\n",
        "\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, \n",
        "                                      restore_best_weights=True,verbose=1)\n",
        "\n",
        "model_2.fit(train_padded, y_train, validation_split=0.1,epochs=EPOCHS, \n",
        "          batch_size=BATCH_SIZE, callbacks=[es], verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 260, 300)          44910300  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 260, 128)          153728    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 130, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 130, 64)           32832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 65, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 65, 32)            8224      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 32, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 45,367,998\n",
            "Trainable params: 457,698\n",
            "Non-trainable params: 44,910,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "247/247 [==============================] - 141s 569ms/step - loss: 0.4317 - accuracy: 0.7834 - val_loss: 0.3119 - val_accuracy: 0.8731\n",
            "Epoch 2/12\n",
            "247/247 [==============================] - 141s 569ms/step - loss: 0.2863 - accuracy: 0.8797 - val_loss: 0.3075 - val_accuracy: 0.8717\n",
            "Epoch 3/12\n",
            "247/247 [==============================] - 140s 565ms/step - loss: 0.2227 - accuracy: 0.9108 - val_loss: 0.3026 - val_accuracy: 0.8766\n",
            "Epoch 4/12\n",
            "247/247 [==============================] - 144s 583ms/step - loss: 0.1546 - accuracy: 0.9413 - val_loss: 0.2969 - val_accuracy: 0.8834\n",
            "Epoch 5/12\n",
            "247/247 [==============================] - 140s 565ms/step - loss: 0.0942 - accuracy: 0.9668 - val_loss: 0.4309 - val_accuracy: 0.8749\n",
            "Epoch 6/12\n",
            "247/247 [==============================] - 141s 569ms/step - loss: 0.0465 - accuracy: 0.9836 - val_loss: 0.4558 - val_accuracy: 0.8820\n",
            "Epoch 7/12\n",
            "247/247 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9879Restoring model weights from the end of the best epoch.\n",
            "247/247 [==============================] - 140s 567ms/step - loss: 0.0357 - accuracy: 0.9879 - val_loss: 0.4814 - val_accuracy: 0.8766\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d81fb4978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj2pELHNNhGR",
        "colab_type": "text"
      },
      "source": [
        "# Dense NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJuK_eXOrcuM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "e5031728-206b-426c-e40b-8e3c3adbed4b"
      },
      "source": [
        "model_3 = Sequential()\n",
        "model_3.add(Embedding(input_dim=vocab_size, output_dim=vec_size,input_length=max_length,\n",
        "weights=[embedding_matrix],trainable=False))\n",
        "model_3.add(Flatten())\n",
        "model_3.add(Dense(256, activation='relu'))\n",
        "model_3.add(Dense(128, activation='relu'))\n",
        "model_3.add(Dense(2, activation='softmax'))\n",
        "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_3.summary()\n",
        "\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, \n",
        "                                      restore_best_weights=True,verbose=1)\n",
        "\n",
        "model_3.fit(train_padded, y_train, validation_split=0.1,epochs=EPOCHS, \n",
        "          batch_size=BATCH_SIZE, callbacks=[es], verbose=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 260, 300)          44910300  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 78000)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               19968256  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 64,911,710\n",
            "Trainable params: 20,001,410\n",
            "Non-trainable params: 44,910,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "247/247 [==============================] - 64s 259ms/step - loss: 0.4862 - accuracy: 0.7667 - val_loss: 0.4233 - val_accuracy: 0.8077\n",
            "Epoch 2/12\n",
            "247/247 [==============================] - 63s 256ms/step - loss: 0.1990 - accuracy: 0.9173 - val_loss: 0.5449 - val_accuracy: 0.7909\n",
            "Epoch 3/12\n",
            "247/247 [==============================] - 63s 256ms/step - loss: 0.0690 - accuracy: 0.9750 - val_loss: 0.8424 - val_accuracy: 0.7846\n",
            "Epoch 4/12\n",
            "247/247 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9890Restoring model weights from the end of the best epoch.\n",
            "247/247 [==============================] - 63s 256ms/step - loss: 0.0322 - accuracy: 0.9890 - val_loss: 1.0379 - val_accuracy: 0.7914\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d814fe7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}